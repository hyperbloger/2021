<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
#线性单元

* 线性单元最重要的特点**是使用一个可导的线性函数代替阶跃函数这类感知器激活函数**。
* 线性单元能做到数据集不能被*一条线性函数* 区分时，能将数据集收敛到一个最佳的近似上。
* 线性函数主要用于线性回归类的问题，**(即数据收敛于一个线性函数)**。返回的输出值是一个实数值。

#机器学习最重要的三个点：
* **模型**
* **目标函数**
* **优化算法**
#模型

##输入 x 预测输出 y 的算法
$$y=h(x)=w*x+b
$$h(x)叫做假设，是模型的计算规则，h(x)有很多特征参与计算，每个特征都是一个样本的必要信息，这样，一个样本可以用一个特征向量表示它。$$x=[c1,c2,...c_n\small]
$$公式计算可以写成向量的形式。$$
y=w^\mathrm{T}*x
$$这种模型符合线性组合的规范，会将它称为线性模型。

#模型的训练
感知器训练规则——权重项和偏置项的训练。
$$
\begin{cases}
有监督学习& \text{x,y}\\
无监督学习& \text{x}
\end{cases}
$$y就是训练集标记的输出值，可以称为label
* *有监督学习*
样本的x,y被提供，算法通过样本的全部特征和真实值训练参数，总结模型对数据(输入，输出)的关系。
* *无监督学习*
样本的x被提供，模型只能通过总结样本特征的规律来接近真实的答案。

面对无监督的庞大样本量
工业界一般采用的技术：
无监督学习中有一类算法称为聚类，如KMeans等。聚类算法能够将相似的样本特征聚在一起，这样庞大的样本特征量能得到压缩，取经过处理的样本和对应的标注输出值的结果再进行训练，得到模型对数据的关联。

#目标函数
目标函数是输出值和预测值误差-**接近程度**的函数表示。当目标函数最小(最大)时,模型中参数是最优的，也意味着样本最接近正确的输出。
**但**目标函数存在多个局部最小(最大)时，不是全局最小(最大),模型参数也只是局部最优解。

数学公式中表示输出值和预测值的接近程度有很多，简单的用一个数学公式来表示它们的误差：
$$e=1/2(y-\overline{y})^2
$$样本的误差可以统计成$$
E=e(1)+e(2)+...+e(n)=\sum_{i=1}^ne(i)=\sum_{i=1}^n1/2(y^{(i)}-\overline{y}^{(i)})^2
$$$$
\overline{y}=h(x)=w*x+b=w^\mathrm{T}*x;
$$偏项值b被分解成w0=0,x0=1; 
$$\overline{y}^{(i)}=w^\mathrm{T}*x^{(i)}
$$**归纳总结：**$$
E=\sum_{i=1}^n1/2(y^{(i)}-\overline{y}^{(i)})^2=1/2\sum_{i=1}^n(y^{(i)}-w^\mathrm{T}*x^{(i)})^2
$$
x(i)表示第i个样本的特征，y(i)表示样本的真实输出值，即label，(x(i),y(i))表示第i个训练样本。在有监督学习下，(x(i),y(i))是已知的，关于目标函数E的函数表示是仅与w相关的(其实是跟权重项和偏置项相关，但偏置项b被分解成了w0)。
**结果**
$$E(w)=1/2\sum_{i=1}^n(y^{(i)}-w^\mathrm{T}*x^{(i)})^2
$$

#优化算法
**优化算法推导出感知器的训练规则**
模型的训练，是为了找到合适的权重项w和偏置项b,使得目标函数的接近程度最高。
寻找最优解，这里不管局部最优还是全局最优，是优化算法。
*优化算法是通过不断地尝试将函数地极值点算出来。*
**优化算法的重要节点：**
梯度**下降**/**上升**优化算法。
找最小值/最大值的过程都是朝着数值下降最快/数值上升最快的方向前进的。
**梯度**是这样的一种表示，它是一个向量，数学意义上表示该点周围最小值向最大值的标识，方向是最小值指向最大值，大小是最大值和最小值的差值，我们称梯度叫数值上升最快的变化量。
梯度下降算法就很好理解，方向相反，大小没有发生改变，但方向从原来的数值上升最快反转180指向了数值下降最快的。
##梯度下降算法的通用公式:
$$X_{new}=X_{old}-\eta\bigtriangledown f(x)
$$结合线性感知器的模型和目标函数，公式转换成：$$W_{new}=W_{old}-\eta\bigtriangledown E(w)
$$$$\bigtriangledown E(w)=-\sum_{i=1}^n(y^{(i)}-\overline{y}^{(i)})x^{(i)}
$$

梯度E(w)的演化过程：
$$
\bigtriangledown E(w)=\partial E(w)/\partial w=\partial1/2\sum_{i=1}^n(y^{(i)}-\overline{y}^{(i)})^2/\partial w
$$$$
1/2\sum_{i=1}^n\partial(y^{(i)}-\overline{y}^{(i)})^2/\partial w
$$*先对偏导数求解*
$$\partial(y^{(i)}-\overline{y}^{(i)})^2/\partial w=\partial(y^{(i)^2}-2y^{(i)}\overline{y}^{(i)}+\overline{y}^{(i)^2})/ \partial w
$$根据复合函数求偏导规则——*链式求导*$$
\partial E(w)/\partial w=\partial E(\overline{y}^{(i)})/\partial \overline{y}^{(i)}*\partial \overline{y}^{(i)}/\partial w
$$公式展开：$$
=(-2y^{(i)}+2\overline{y}^{(i)})*\partial(w^\mathrm{T}*x)/ \partial w
$$$$=(-2y^{(i)}+2\overline{y}^{(i)})*x
$$
**结果是：**$$-\sum_{i=1}^n(y^{(i)}-\overline{y}^{(i)})*x
$$线性分类的计算公式也基本上是这么得到的。
$$W_{new}=W_{old}-\bigtriangledown E(W)=W_{old}+\sum_{i=1}^n(y^{(i)}-\overline{y}^{(i)})*x
$$

#启示录 
##批梯度下降算法和随机梯度下降算法(Stochastic Gradient Descent)

SGD广泛使用的原因:
1. 样本体量大的时候计算效率高，因为一个迭代只取一个样本，批梯度可能一次计算要上百万次更新。
2. 当面对目标函数有多个局部最优的解，SGD可以帮助获取更好的一个模型。批梯度则逃离不掉这种局面。

SGD提高了效率，但随机带来的噪音使得更新w并不一定按照目标函数最优解的方向前进，但好在大量的更新总体上还是沿着目标函数优化方向前进，最终也能收敛到最小值附近。
